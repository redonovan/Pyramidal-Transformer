# Pyramidal-Transformer
<B>Research</B>

April 2022 : This page needs to be revisited now that I have got my <a href="https://github.com/redonovan/LAS-Variations">LAS system working properly</a> using curriculum learning.

<P>

<a href="https://arxiv.org/abs/1508.01211">Listen, Attend and Spell</a> was published in 2015, before the invention of the Transformer, and so it is natural to wonder whether using a Transformer encoder in place of the bidirectional LSTM encoder in the Listener would be helpful; doubly so since I read <a href="https://arxiv.org/abs/1910.09799">Wang, et al. 2020</a> which found Transformer encoders to be useful in Hybrid systems.

<P>

  My code for the Transformer version of the system is in <a href="TransformAttendSpell.py">TransformAttendSpell.py</a>.  Analysis showed that the multi-head-attention softmax logits in the first layer of the Transformer were too large (close to 200) and so an extra normalization layer was introduced after the positional encoding addition.  Attention logit scaling in the decoder was used, as in my <a href="https://github.com/redonovan/LAS-Variations">LAS Variations</a>, but a monotonicity loss was not.  The Transformer based Listener led to a larger memory requirement during training necessitating a batch_size of 1.  Gradient norm clipping was introduced to stabilise training.  Training was slower, with only 1 epoch of training being possible in the 8-9 hrs I allow for training.  The TensorBoard training curve is in <a href="TransformTensorBoard.png">TransformTensorBoard.png</a>.  Validation data next-character prediction accuracy at the end of training was 0.4889, compared to the corresponding LAS Variant results of 0.4495 and 0.5517 after 1 epoch and 8-9 hours of training respectively.  Unlike the LAS Variant however, the decoder <a href="TransformAttentionWeights.png">attention weights</a> were essentially uniform over acoustic timesteps for all characters, due to the <a href="TransformListenerRepresentation.png">Listener representation (h)</a> being essentially identical over acoustic timesteps, suggesting the Transformer based Listener had not (yet) learned to encode the acoustic information into a useful representation.  The LAS Variant had 8.5m weights in its Listener, while the Transformer version had 9.5m.

<P>

  It was noted in the original Listen, Attend and Spell paper that the pyramid structure was necessary for the model to converge in reasonable timeframes.  One reason the Transformer version of LAS described above did not out-perform my original BLSTM version might therefore be that it lacks this pyramidal structure.  To investigate this possibility I implemented a pyramidal version in which each Transformer layer reshapes the output of its self-attention sub-layer to halve the number of timesteps, doubling the dimensionality, and then uses the pointwise feed-forward sub-layer to reduce the dimensionality back to its original value.  My code is in <a href="PyTranAttendSpell.py">PyTranAttendSpell.py</a>.  This system could be run with a batch_size of 4, and was trained for 3 epochs over 8 hours.  The Tensorboard training curve is in <a href="PyTranTensorBoard.png">PyTranTensorBoard.png</a>.  Validation data next-character prediction results were 0.4994 and 0.5287 accuracy after 1 epoch and 8 hours respectively.  As with the Transformer system, the decoder <a href="PyTranAttentionWeights.png">attention weights</a> were essentially uniform over acoustic timesteps for all characters, due to the <a href="PyTranListenerRepresentation.png">Listener representation (h)</a> being essentially identical over timesteps.  In this system the Transformer pointwise feed-forward network intermediate dimension was reduced to 1408 so as to have approximately the same number of weights (9.7m) in the Listener as the Transformer system.  
<P> 

  Neither my Transformer system nor my Pyramidal Transformer system out-performed the corresponding LAS Variant.  In both cases the Listeners do not appear to have learned to encode the acoustic information into a useful representation.  Further analysis showed that the representations produced by the first layer of these systems' 3-layer Listeners did vary over timesteps, but by the 3rd layer that variation was gone.  This is in contrast to the corresponding LAS Variant which does preserve some variation over timesteps into its <a href="LASVarListenerRepresentation.png">3rd layer representation</a>, though even this is surprisingly poor.  Future experiments might therefore involve training 1 or 2-layer Listeners, and/or using frame stacking to reduce both timesteps and memory requirements allowing for larger batch sizes and more rapid training.  The Transformer version might benefit from greedy layer-wise pretraining, or an iterated loss as used in <a href="https://arxiv.org/abs/1910.09799">Wang, et al. 2020</a>, to encourage the learning of useful representations at each layer.  A more localised form of attention might better preserve Listener variation over timesteps.  Finally, of course, more training data and more training time might be helpful.
